"""
Thor VLM æ¨ç†æœåŠ¡å™¨ - åŸºäºQwen2.5-VL-3B-Instruct
åŠŸèƒ½: æ¥æ”¶G1å‘é€çš„å›¾åƒå’Œæ–‡æœ¬ï¼Œè¿›è¡ŒVLMæ¨ç†ï¼Œè¿”å›å“åº”
æ¨¡å‹: Qwen2.5-VL-3B-Instruct (äººç‰©åˆ†æã€æƒ…æ„Ÿè¯†åˆ«)
"""

import json
import logging
import base64
import numpy as np
import cv2
import torch
import io
import re
from typing import Optional
from PIL import Image

from unitree_sdk2py.core.channel import ChannelFactoryInitialize, ChannelSubscriber, ChannelPublisher
from unitree_sdk2py.idl.std_msgs.msg.dds_._String_ import String_

from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

# ============================================================
# é…ç½®æ—¥å¿—
# ============================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# ============================================================
# é…ç½®å‚æ•°
# ============================================================
NETWORK_INTERFACE = "eth0"  # Thorçš„ç½‘ç»œæ¥å£ï¼Œæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
RECV_TOPIC = "rt/thor_request"   # æ¥æ”¶G1è¯·æ±‚çš„è¯é¢˜
SEND_TOPIC = "rt/thor_response"  # å‘é€å“åº”çš„è¯é¢˜

# Qwen2.5-VL æ¨¡å‹é…ç½®
MODEL_PATH = "/home/bryce/models/Qwen2.5-VL-3B-Instruct"
IMAGE_HEIGHT = 280
IMAGE_WIDTH = 420
PIXELS = IMAGE_HEIGHT * IMAGE_WIDTH


# ============================================================
# ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰
# ============================================================
class ContextManager:
    """
    å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    ä¿æŒæœ€è¿‘Nè½®çš„å¯¹è¯å†å²ï¼Œæ”¯æŒè¿è´¯å¯¹è¯
    """
    
    def __init__(self, max_turns: int = 10):
        """
        åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        
        å‚æ•°:
            max_turns: æœ€å¤§ä¿ç•™çš„å¯¹è¯è½®æ•°ï¼ˆé»˜è®¤10è½®ï¼‰
        """
        self.max_turns = max_turns
        self.history = []  # å¯¹è¯å†å²: [{"role": "user"/"assistant", "content": "..."}]
        logger.info(f"ğŸ“š ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆå§‹åŒ–: æœ€å¤§ä¿ç•™ {max_turns} è½®å¯¹è¯")
    
    def add_user(self, text: str):
        """æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²"""
        self.history.append({"role": "user", "content": text})
        self._trim()
        logger.debug(f"â• æ·»åŠ ç”¨æˆ·æ¶ˆæ¯: {text[:50]}...")
    
    def add_assistant(self, text: str):
        """æ·»åŠ æœºå™¨äººå›å¤åˆ°å†å²"""
        self.history.append({"role": "assistant", "content": text})
        self._trim()
        logger.debug(f"â• æ·»åŠ æœºå™¨äººå›å¤: {text[:50]}...")
    
    def _trim(self):
        """ä¿æŒå†å²åœ¨æœ€å¤§è½®æ•°å†…"""
        if len(self.history) > self.max_turns * 2:  # æ¯è½®åŒ…å«user+assistant
            removed = len(self.history) - self.max_turns * 2
            self.history = self.history[-self.max_turns * 2:]
            logger.debug(f"ğŸ—‘ï¸  ç§»é™¤æœ€æ—©çš„ {removed} æ¡æ¶ˆæ¯")
    
    def get_history(self) -> list:
        """è·å–å½“å‰å¯¹è¯å†å²"""
        return self.history.copy()
    
    def get_context_summary(self) -> str:
        """è·å–ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆç”¨äºæ—¥å¿—ï¼‰"""
        if not self.history:
            return "æ— å†å²å¯¹è¯"
        turns = len(self.history) // 2
        return f"{turns}è½®å¯¹è¯ï¼Œæœ€è¿‘: {self.history[-1]['content'][:30]}..."
    
    def clear(self):
        """æ¸…ç©ºå†å²ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        self.history = []
        logger.info("ğŸ—‘ï¸  ä¸Šä¸‹æ–‡å·²æ¸…ç©º")


# ============================================================
# Qwen VLM æ¨¡å‹å°è£…
# ============================================================
class QwenVLMModel:
    """
    Qwen2.5-VL æ¨¡å‹å°è£…
    æ”¯æŒå›¾åƒ+æ–‡æœ¬è¾“å…¥ï¼Œè¾“å‡ºç»“æ„åŒ–JSONå“åº”
    æ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡
    """
    
    def __init__(self, model_path: str = MODEL_PATH):
        """
        åˆå§‹åŒ–Qwen VLMæ¨¡å‹
        
        å‚æ•°:
            model_path: Qwen2.5-VLæ¨¡å‹è·¯å¾„
        """
        logger.info("ğŸ”§ æ­£åœ¨åŠ è½½ Qwen2.5-VL-3B-Instruct æ¨¡å‹...")
        logger.info(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {model_path}")
        
        # è®¾ç½®æ•°æ®ç±»å‹
        self.dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        logger.info(f"ğŸ–¥ï¸  è®¾å¤‡: {self.device}, æ•°æ®ç±»å‹: {self.dtype}")
        
        # åŠ è½½processor
        logger.info("ğŸ“¦ åŠ è½½ Processor...")
        self.processor = AutoProcessor.from_pretrained(
            model_path,
            min_pixels=PIXELS,
            max_pixels=PIXELS,
            trust_remote_code=True,
            use_fast=False
        )
        
        # åŠ è½½æ¨¡å‹
        logger.info("ğŸ§  åŠ è½½ VLM æ¨¡å‹...")
        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=self.dtype,
            device_map="auto",
            trust_remote_code=True
        ).eval()
        
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        self.context = ContextManager(max_turns=10)
        
        logger.info("âœ… Qwen2.5-VL æ¨¡å‹åŠ è½½å®Œæˆï¼")
    
    def _parse_json_response(self, text: str) -> dict:
        """
        ä»æ¨¡å‹è¾“å‡ºä¸­æå–JSON
        
        å‚æ•°:
            text: æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬
        
        è¿”å›:
            è§£æåçš„å­—å…¸
        """
        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(text)
        except json.JSONDecodeError:
            # å°è¯•æå–JSONéƒ¨åˆ†
            # æŸ¥æ‰¾ {...} æ¨¡å¼
            json_match = re.search(r'\{[^}]+\}', text)
            if json_match:
                try:
                    return json.loads(json_match.group())
                except:
                    pass
            
            # è§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æ„
            logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥ï¼ŒåŸå§‹è¾“å‡º: {text}")
            return {
                "age": 25,
                "gender": "unknown",
                "emotion": "neutral",
                "raw_output": text
            }
    
    def _determine_action_and_response(self, analysis: dict, user_text: str) -> dict:
        """
        æ ¹æ®åˆ†æç»“æœå’Œç”¨æˆ·è¾“å…¥ï¼Œå†³å®šæœºå™¨äººçš„å›å¤å’ŒåŠ¨ä½œ
        
        å‚æ•°:
            analysis: VLMåˆ†æç»“æœ {"age": int, "gender": str, "emotion": str}
            user_text: ç”¨æˆ·è¯´çš„è¯
        
        è¿”å›:
            {
                "response_text": str,    # æœºå™¨äººè¦è¯´çš„è¯
                "action": str,           # åŠ¨ä½œåç§°
                "action_type": str,      # åŠ¨ä½œç±»å‹
                "robot_emotion": str     # æœºå™¨äººè¡¨è¾¾çš„æƒ…æ„Ÿ
            }
        """
        age = analysis.get("age", 25)
        gender = analysis.get("gender", "unknown")
        emotion = analysis.get("emotion", "neutral")
        
        # æ ¹æ®ç”¨æˆ·æƒ…æ„Ÿå†³å®šæœºå™¨äººå“åº”
        emotion_lower = emotion.lower()
        
        # æƒ…æ„Ÿæ˜ å°„åˆ°å›å¤å’ŒåŠ¨ä½œ
        if "happy" in emotion_lower or "joy" in emotion_lower or "smile" in emotion_lower:
            response_text = f"ä½ çœ‹èµ·æ¥å¾ˆå¼€å¿ƒå‘¢ï¼è®©æˆ‘ä»¬ä¸€èµ·å¼€å¿ƒå§ã€‚"
            action = "wave"
            action_type = "gesture"
            robot_emotion = "happy"
        
        elif "sad" in emotion_lower or "unhappy" in emotion_lower or "down" in emotion_lower:
            response_text = f"ä½ çœ‹èµ·æ¥æœ‰ç‚¹ä¸å¼€å¿ƒï¼Œéœ€è¦æˆ‘åšç‚¹ä»€ä¹ˆè®©ä½ å¼€å¿ƒå—ï¼Ÿ"
            action = "nod"
            action_type = "gesture"
            robot_emotion = "concerned"
        
        elif "angry" in emotion_lower or "mad" in emotion_lower:
            response_text = f"æˆ‘æ„Ÿè§‰åˆ°ä½ æœ‰ç‚¹ç”Ÿæ°”ï¼Œè®©æˆ‘ä»¬å†·é™ä¸€ä¸‹å§ã€‚"
            action = "bow"
            action_type = "gesture"
            robot_emotion = "apologetic"
        
        elif "surprise" in emotion_lower or "shocked" in emotion_lower:
            response_text = f"å“‡ï¼Œçœ‹èµ·æ¥å‘ç”Ÿäº†ä»€ä¹ˆè®©ä½ æƒŠè®¶çš„äº‹æƒ…ï¼"
            action = "thumbs_up"
            action_type = "gesture"
            robot_emotion = "excited"
        
        else:  # neutral or other
            response_text = f"ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚"
            action = "wave"
            action_type = "gesture"
            robot_emotion = "friendly"
        
        # æ ¹æ®ç”¨æˆ·è¯´çš„è¯è¿›ä¸€æ­¥è°ƒæ•´
        user_lower = user_text.lower()
        
        if "ä½ å¥½" in user_text or "hello" in user_lower or "hi" in user_lower:
            response_text = f"ä½ å¥½ï¼æˆ‘çœ‹åˆ°ä½ äº†ï¼Œ{gender_text(gender)}ã€‚" + response_text
            action = "wave"
        
        elif "å‰è¿›" in user_text or "forward" in user_lower or "èµ°" in user_text:
            response_text = "å¥½çš„ï¼Œæˆ‘ç°åœ¨å‘å‰ç§»åŠ¨ã€‚"
            action = "forward"
            action_type = "movement"
        
        elif "åœ" in user_text or "stop" in user_lower:
            response_text = "å¥½çš„ï¼Œæˆ‘åœä¸‹æ¥äº†ã€‚"
            action = "stop"
            action_type = "system"
        
        elif "æŒ¥æ‰‹" in user_text or "wave" in user_lower:
            response_text = "å¥½çš„ï¼Œæˆ‘å‘ä½ æŒ¥æ‰‹ï¼"
            action = "wave"
            action_type = "gesture"
        
        elif "ç‚¹å¤´" in user_text or "nod" in user_lower:
            response_text = "æ˜ç™½äº†ï¼"
            action = "nod"
            action_type = "gesture"
        
        return {
            "response_text": response_text,
            "action": action,
            "action_type": action_type,
            "robot_emotion": robot_emotion
        }
    
    def inference(self, image: np.ndarray, text: str) -> dict:
        """
        æ‰§è¡ŒVLMæ¨ç†ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
        
        å‚æ•°:
            image: OpenCVæ ¼å¼çš„å›¾åƒ (BGR, numpy array)
            text: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        
        è¿”å›:
            {
                "response_text": "æœºå™¨äººè¦è¯´çš„è¯",
                "action": "wave",
                "action_type": "gesture",
                "emotion": "happy",
                "confidence": 0.95,
                "analysis": {"age": 25, "gender": "male", "emotion": "happy"}
            }
        """
        logger.info(f"ğŸ§  å¼€å§‹VLMæ¨ç†ï¼ˆå¤šè½®å¯¹è¯ï¼‰...")
        logger.info(f"ğŸ“ ç”¨æˆ·è¾“å…¥: '{text}'")
        logger.info(f"ğŸ“š å½“å‰ä¸Šä¸‹æ–‡: {self.context.get_context_summary()}")
        logger.info(f"ğŸ“· å›¾åƒå°ºå¯¸: {image.shape}")
        
        try:
            # 1. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            self.context.add_user(text)
            
            # 2. è½¬æ¢å›¾åƒæ ¼å¼ (OpenCV BGR -> PIL RGB)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(image_rgb)
            
            # 3. ä¿å­˜ä¸´æ—¶å›¾åƒï¼ˆQwenéœ€è¦æ–‡ä»¶è·¯å¾„ï¼‰
            temp_image_path = "/tmp/thor_temp_image.jpg"
            pil_image.save(temp_image_path)
            img_path = f"file://{temp_image_path}"
            
            # 4. æ„å»ºprompt - åŒ…å«ä¸Šä¸‹æ–‡å’Œå½“å‰ä»»åŠ¡
            history_context = ""
            if self.context.history:
                history_context = "\n\n--- Recent Conversation History ---\n"
                for msg in self.context.history[-6:]:  # æœ€è¿‘3è½®ï¼ˆ6æ¡æ¶ˆæ¯ï¼‰
                    role = "User" if msg["role"] == "user" else "Robot"
                    history_context += f"{role}: {msg['content']}\n"
                history_context += "--- End of History ---\n\n"
            
            query = (
                f"{history_context}"
                f"Current user input: '{text}'\n\n"
                "Please analyze the person in this image. "
                "Estimate their approximate age (in years), gender, and emotional state "
                "based on visual cues. "
                "Output the result in JSON format as: "
                "{\"age\": <int>, \"gender\": <male/female>, \"emotion\": <string>}."
            )
            
            # 5. æ„å»ºæ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "image": img_path,
                            "resized_height": IMAGE_HEIGHT,
                            "resized_width": IMAGE_WIDTH
                        },
                        {"type": "text", "text": query}
                    ]
                }
            ]
            
            # 6. å¤„ç†è¾“å…¥
            text_prompt = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            image_inputs, video_inputs = process_vision_info(messages)
            inputs = self.processor(
                text=[text_prompt],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt"
            ).to(self.device)
            
            # 7. æ¨ç†
            logger.info("âš¡ æ‰§è¡Œæ¨¡å‹æ¨ç†...")
            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=256,
                    do_sample=False
                )
            
            # 8. è§£ç è¾“å‡º
            trimmed = [o[len(i):] for i, o in zip(inputs.input_ids, outputs)]
            output_text = self.processor.batch_decode(
                trimmed, skip_special_tokens=True
            )[0]
            
            logger.info(f"ğŸ“¤ æ¨¡å‹åŸå§‹è¾“å‡º: {output_text}")
            
            # 9. è§£æJSON
            analysis = self._parse_json_response(output_text)
            logger.info(f"ğŸ“Š åˆ†æç»“æœ: {analysis}")
            
            # 10. å†³å®šæœºå™¨äººçš„å›å¤å’ŒåŠ¨ä½œï¼ˆè€ƒè™‘ä¸Šä¸‹æ–‡ï¼‰
            decision = self._determine_action_and_response(analysis, text)
            
            # 11. æ·»åŠ æœºå™¨äººå›å¤åˆ°å†å²
            self.context.add_assistant(decision["response_text"])
            
            # 12. æ„å»ºæœ€ç»ˆå“åº”
            result = {
                "response_text": decision["response_text"],
                "action": decision["action"],
                "action_type": decision["action_type"],
                "emotion": decision["robot_emotion"],
                "confidence": 0.90,
                "analysis": analysis
            }
            
            logger.info(f"âœ… æ¨ç†å®Œæˆ: {result}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ VLMæ¨ç†å¤±è´¥: {e}", exc_info=True)
            # è¿”å›é»˜è®¤å“åº”
            return {
                "response_text": f"æˆ‘å¬åˆ°ä½ è¯´ï¼š{text}",
                "action": "nod",
                "action_type": "gesture",
                "emotion": "neutral",
                "confidence": 0.5,
                "error": str(e)
            }


def gender_text(gender: str) -> str:
    """æ€§åˆ«æ–‡æœ¬è½¬æ¢"""
    if gender == "male":
        return "å…ˆç”Ÿ"
    elif gender == "female":
        return "å¥³å£«"
    else:
        return "æœ‹å‹"


# ============================================================
# ThoræœåŠ¡å™¨ä¸»ç±»
# ============================================================
class ThorVLMServer:
    """Thor VLMæ¨ç†æœåŠ¡å™¨"""
    
    def __init__(self, network_interface: str):
        self.network_interface = network_interface
        self.vlm_model = QwenVLMModel()
        self.subscriber: Optional[ChannelSubscriber] = None
        self.publisher: Optional[ChannelPublisher] = None
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–ROS2é€šä¿¡"""
        try:
            logger.info("=" * 60)
            logger.info("ğŸ–¥ï¸  Thor VLMæœåŠ¡å™¨ - æ­£åœ¨åˆå§‹åŒ–")
            logger.info("=" * 60)
            
            # åˆå§‹åŒ–ROS2 DDS
            logger.info(f"ğŸ“¡ åˆå§‹åŒ–ROS2 DDSï¼Œç½‘ç»œæ¥å£: {self.network_interface}")
            ChannelFactoryInitialize(0, self.network_interface)
            
            # åˆ›å»ºè®¢é˜…è€…ï¼ˆæ¥æ”¶G1çš„è¯·æ±‚ï¼‰
            logger.info(f"ğŸ“¥ è®¢é˜…è¯é¢˜: {RECV_TOPIC}")
            self.subscriber = ChannelSubscriber(RECV_TOPIC, String_)
            self.subscriber.Init(self._on_request)
            
            # åˆ›å»ºå‘å¸ƒè€…ï¼ˆå‘é€å“åº”ç»™G1ï¼‰
            logger.info(f"ğŸ“¤ åˆ›å»ºå‘å¸ƒè€…: {SEND_TOPIC}")
            self.publisher = ChannelPublisher(SEND_TOPIC, String_)
            self.publisher.Init()
            
            logger.info("=" * 60)
            logger.info("âœ… Thor VLMæœåŠ¡å™¨åˆå§‹åŒ–æˆåŠŸï¼")
            logger.info("=" * 60)
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            return False
    
    def _on_request(self, msg: String_) -> None:
        """
        å¤„ç†G1å‘æ¥çš„æ¨ç†è¯·æ±‚
        """
        try:
            # è§£æJSONæ•°æ®
            raw_data = msg.data if isinstance(msg.data, str) else msg.data()
            data = json.loads(raw_data)
            
            text = data.get("text", data.get("asr_text", ""))
            image_b64 = data.get("image_base64", "")
            request_id = data.get("request_id", "unknown")
            timestamp = data.get("timestamp", 0)
            
            logger.info("=" * 60)
            logger.info(f"ğŸ“¨ æ”¶åˆ°è¯·æ±‚ (ID: {request_id[:8]}...)")
            logger.info(f"ğŸ“ æ–‡æœ¬: '{text}'")
            logger.info(f"â±ï¸  æ—¶é—´æˆ³: {timestamp}")
            
            # è§£ç å›¾åƒ
            image = None
            if image_b64:
                try:
                    img_data = base64.b64decode(image_b64)
                    nparr = np.frombuffer(img_data, np.uint8)
                    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                    logger.info(f"ğŸ“· å›¾åƒè§£ç æˆåŠŸ: {image.shape}")
                except Exception as e:
                    logger.warning(f"âš ï¸ å›¾åƒè§£ç å¤±è´¥: {e}")
                    # ä½¿ç”¨ç©ºç™½å›¾åƒ
                    image = np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH, 3), dtype=np.uint8)
            else:
                logger.warning("âš ï¸ æœªæ”¶åˆ°å›¾åƒï¼Œä½¿ç”¨ç©ºç™½å›¾åƒ")
                image = np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH, 3), dtype=np.uint8)
            
            # VLMæ¨ç†
            result = self.vlm_model.inference(image, text)
            
            # æ„å»ºå“åº”
            response = {
                "status": "success",
                "text": result["response_text"],
                "action": result["action"],
                "action_type": result["action_type"],
                "emotion": result["emotion"],
                "confidence": result["confidence"],
                "request_id": request_id,
                "analysis": result.get("analysis", {})
            }
            
            # å‘é€å“åº”
            self._send_response(response)
            logger.info(f"âœ… å“åº”å·²å‘é€ (ID: {request_id[:8]}...)")
            logger.info("=" * 60)
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
            self._send_error_response("json_parse_error", request_id="unknown")
        except Exception as e:
            logger.error(f"âŒ è¯·æ±‚å¤„ç†å¤±è´¥: {e}", exc_info=True)
            self._send_error_response(
                str(e),
                request_id=data.get("request_id", "unknown") if 'data' in locals() else "unknown"
            )
    
    def _send_response(self, response: dict) -> None:
        """å‘é€å“åº”ç»™G1"""
        try:
            msg = String_()
            msg.data = json.dumps(response, ensure_ascii=False)
            self.publisher.Write(msg)
            logger.debug(f"ğŸ“¡ å‘é€å“åº”: {json.dumps(response, ensure_ascii=False)[:200]}...")
        except Exception as e:
            logger.error(f"âŒ å‘é€å“åº”å¤±è´¥: {e}")
    
    def _send_error_response(self, error_msg: str, request_id: str) -> None:
        """å‘é€é”™è¯¯å“åº”"""
        response = {
            "status": "error",
            "error": error_msg,
            "text": "æŠ±æ­‰ï¼Œå¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ã€‚",
            "request_id": request_id,
            "action": "shake_head",
            "action_type": "gesture",
            "confidence": 0.0
        }
        self._send_response(response)
    
    def run(self):
        """è¿è¡ŒæœåŠ¡å™¨ï¼ˆä¿æŒè¿è¡ŒçŠ¶æ€ï¼‰"""
        logger.info("=" * 60)
        logger.info("ğŸš€ Thor VLMæœåŠ¡å™¨ - è¿è¡Œä¸­")
        logger.info("ğŸ¯ ç­‰å¾…G1è¯·æ±‚...")
        logger.info("âŒ¨ï¸  æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
        logger.info("=" * 60)
        
        try:
            import time
            while True:
                time.sleep(0.1)  # ä¸»å¾ªç¯ï¼ŒROS2å›è°ƒåœ¨åå°çº¿ç¨‹å¤„ç†
        except KeyboardInterrupt:
            logger.info("\nâ¹ï¸  æ”¶åˆ°åœæ­¢ä¿¡å· (Ctrl+C)")
            self.stop()
    
    def stop(self):
        """åœæ­¢æœåŠ¡å™¨"""
        logger.info("=" * 60)
        logger.info("ğŸ›‘ æ­£åœ¨åœæ­¢ Thor VLMæœåŠ¡å™¨")
        logger.info("=" * 60)
        # æ¸…ç†èµ„æº
        if self.subscriber:
            self.subscriber = None
        if self.publisher:
            self.publisher = None
        logger.info("âœ… Thor VLMæœåŠ¡å™¨å·²åœæ­¢")


# ============================================================
# ä¸»ç¨‹åºå…¥å£
# ============================================================
def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Thor VLMæ¨ç†æœåŠ¡å™¨ (åŸºäºQwen2.5-VL-3B-Instruct)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python thor_vlm_server.py eth0
  python thor_vlm_server.py wlan0 --debug

æ³¨æ„:
  - ç¡®ä¿Qwen2.5-VLæ¨¡å‹è·¯å¾„æ­£ç¡®: {MODEL_PATH}
  - ç¡®ä¿ä¸G1æœºå™¨äººåœ¨åŒä¸€ç½‘ç»œ
  - æ¨èä½¿ç”¨GPUåŠ é€Ÿ (CUDA)
        """.format(MODEL_PATH=MODEL_PATH)
    )
    
    parser.add_argument(
        "network_interface",
        help="ç½‘ç»œæ¥å£åç§° (ä¾‹å¦‚: eth0, wlan0)",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¼€å¯è°ƒè¯•æ—¥å¿—",
    )
    
    args = parser.parse_args()
    
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("ğŸ› è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    logger.info("=" * 60)
    logger.info("ğŸ”§ ç³»ç»Ÿä¿¡æ¯")
    logger.info("=" * 60)
    logger.info(f"ğŸ–¥ï¸  CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        logger.info(f"ğŸ® GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
        logger.info(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    logger.info(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    logger.info(f"ğŸ“ å›¾åƒå°ºå¯¸: {IMAGE_WIDTH}x{IMAGE_HEIGHT}")
    logger.info("=" * 60)
    
    # åˆ›å»ºå¹¶è¿è¡ŒæœåŠ¡å™¨
    server = ThorVLMServer(args.network_interface)
    
    if not server.initialize():
        logger.error("âŒ åˆå§‹åŒ–å¤±è´¥")
        return 1
    
    server.run()
    return 0


if __name__ == "__main__":
    exit(main())
