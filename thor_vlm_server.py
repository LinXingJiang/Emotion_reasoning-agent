"""
Thor VLM æ¨ç†æœåŠ¡å™¨ - åŸºäºQwen2.5-VL-3B-Instruct
åŠŸèƒ½: æ¥æ”¶G1å‘é€çš„å›¾åƒå’Œæ–‡æœ¬ï¼Œè¿›è¡ŒVLMæ¨ç†ï¼Œè¿”å›å“åº”
æ¨¡å‹: Qwen2.5-VL-3B-Instruct (äººç‰©åˆ†æã€æƒ…æ„Ÿè¯†åˆ«)
é€šä¿¡: HTTP/REST API (Flask)
"""

import json
import logging
import base64
import numpy as np
import cv2
import torch
import io
import re
from typing import Optional
from PIL import Image
from flask import Flask, request, jsonify

from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

# ============================================================
# é…ç½®æ—¥å¿—
# ============================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# ============================================================
# é…ç½®å‚æ•°
# ============================================================
# HTTPæœåŠ¡å™¨é…ç½®
HOST = "0.0.0.0"  # ç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£
PORT = 5000       # æœåŠ¡ç«¯å£

# Qwen2.5-VL æ¨¡å‹é…ç½®
MODEL_PATH = "/home/bryce/models/Qwen2.5-VL-3B-Instruct"
IMAGE_HEIGHT = 280
IMAGE_WIDTH = 420
PIXELS = IMAGE_HEIGHT * IMAGE_WIDTH


# ============================================================
# ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰
# ============================================================
class ContextManager:
    """
    å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    ä¿æŒæœ€è¿‘Nè½®çš„å¯¹è¯å†å²ï¼Œæ”¯æŒè¿è´¯å¯¹è¯
    """
    
    def __init__(self, max_turns: int = 10):
        """
        åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        
        å‚æ•°:
            max_turns: æœ€å¤§ä¿ç•™çš„å¯¹è¯è½®æ•°ï¼ˆé»˜è®¤10è½®ï¼‰
        """
        self.max_turns = max_turns
        self.history = []  # å¯¹è¯å†å²: [{"role": "user"/"assistant", "content": "..."}]
        logger.info(f"ğŸ“š ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆå§‹åŒ–: æœ€å¤§ä¿ç•™ {max_turns} è½®å¯¹è¯")
    
    def add_user(self, text: str):
        """æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²"""
        self.history.append({"role": "user", "content": text})
        self._trim()
        logger.debug(f"â• æ·»åŠ ç”¨æˆ·æ¶ˆæ¯: {text[:50]}...")
    
    def add_assistant(self, text: str):
        """æ·»åŠ æœºå™¨äººå›å¤åˆ°å†å²"""
        self.history.append({"role": "assistant", "content": text})
        self._trim()
        logger.debug(f"â• æ·»åŠ æœºå™¨äººå›å¤: {text[:50]}...")
    
    def _trim(self):
        """ä¿æŒå†å²åœ¨æœ€å¤§è½®æ•°å†…"""
        if len(self.history) > self.max_turns * 2:  # æ¯è½®åŒ…å«user+assistant
            removed = len(self.history) - self.max_turns * 2
            self.history = self.history[-self.max_turns * 2:]
            logger.debug(f"ğŸ—‘ï¸  ç§»é™¤æœ€æ—©çš„ {removed} æ¡æ¶ˆæ¯")
    
    def get_history(self) -> list:
        """è·å–å½“å‰å¯¹è¯å†å²"""
        return self.history.copy()
    
    def get_context_summary(self) -> str:
        """è·å–ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆç”¨äºæ—¥å¿—ï¼‰"""
        if not self.history:
            return "æ— å†å²å¯¹è¯"
        turns = len(self.history) // 2
        return f"{turns}è½®å¯¹è¯ï¼Œæœ€è¿‘: {self.history[-1]['content'][:30]}..."
    
    def clear(self):
        """æ¸…ç©ºå†å²ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        self.history = []
        logger.info("ğŸ—‘ï¸  ä¸Šä¸‹æ–‡å·²æ¸…ç©º")


# ============================================================
# Qwen VLM æ¨¡å‹å°è£…
# ============================================================
class QwenVLMModel:
    """
    Qwen2.5-VL æ¨¡å‹å°è£…
    æ”¯æŒå›¾åƒ+æ–‡æœ¬è¾“å…¥ï¼Œè¾“å‡ºç»“æ„åŒ–JSONå“åº”
    æ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡
    """
    
    def __init__(self, model_path: str = MODEL_PATH):
        """
        åˆå§‹åŒ–Qwen VLMæ¨¡å‹
        
        å‚æ•°:
            model_path: Qwen2.5-VLæ¨¡å‹è·¯å¾„
        """
        logger.info("ğŸ”§ æ­£åœ¨åŠ è½½ Qwen2.5-VL-3B-Instruct æ¨¡å‹...")
        logger.info(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {model_path}")
        
        # è®¾ç½®æ•°æ®ç±»å‹
        self.dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        logger.info(f"ğŸ–¥ï¸  è®¾å¤‡: {self.device}, æ•°æ®ç±»å‹: {self.dtype}")
        
        # åŠ è½½processor
        logger.info("ğŸ“¦ åŠ è½½ Processor...")
        self.processor = AutoProcessor.from_pretrained(
            model_path,
            min_pixels=PIXELS,
            max_pixels=PIXELS,
            trust_remote_code=True,
            use_fast=False
        )
        
        # åŠ è½½æ¨¡å‹
        logger.info("ğŸ§  åŠ è½½ VLM æ¨¡å‹...")
        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=self.dtype,
            device_map="auto",
            trust_remote_code=True
        ).eval()
        
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        self.context = ContextManager(max_turns=10)
        
        logger.info("âœ… Qwen2.5-VL æ¨¡å‹åŠ è½½å®Œæˆï¼")
    
    def _parse_json_response(self, text: str) -> dict:
        """
        ä»æ¨¡å‹è¾“å‡ºä¸­æå–JSON
        
        å‚æ•°:
            text: æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬
        
        è¿”å›:
            è§£æåçš„å­—å…¸
        """
        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(text)
        except json.JSONDecodeError:
            # å°è¯•æå–JSONéƒ¨åˆ†
            # æŸ¥æ‰¾ {...} æ¨¡å¼
            json_match = re.search(r'\{[^}]+\}', text)
            if json_match:
                try:
                    return json.loads(json_match.group())
                except:
                    pass
            
            # è§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æ„
            logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥ï¼ŒåŸå§‹è¾“å‡º: {text}")
            return {
                "age": 25,
                "gender": "unknown",
                "emotion": "neutral",
                "raw_output": text
            }
    
    def _determine_action_and_response(self, analysis: dict, user_text: str) -> dict:
        """
        æ ¹æ®åˆ†æç»“æœå’Œç”¨æˆ·è¾“å…¥ï¼Œå†³å®šæœºå™¨äººçš„å›å¤å’ŒåŠ¨ä½œ
        
        å‚æ•°:
            analysis: VLMåˆ†æç»“æœ {"age": int, "gender": str, "emotion": str}
            user_text: ç”¨æˆ·è¯´çš„è¯
        
        è¿”å›:
            {
                "response_text": str,    # æœºå™¨äººè¦è¯´çš„è¯
                "action": str,           # åŠ¨ä½œåç§°
                "action_type": str,      # åŠ¨ä½œç±»å‹
                "robot_emotion": str     # æœºå™¨äººè¡¨è¾¾çš„æƒ…æ„Ÿ
            }
        """
        age = analysis.get("age", 25)
        gender = analysis.get("gender", "unknown")
        emotion = analysis.get("emotion", "neutral")
        
        # æ ¹æ®ç”¨æˆ·æƒ…æ„Ÿå†³å®šæœºå™¨äººå“åº”
        emotion_lower = emotion.lower()
        
        # æƒ…æ„Ÿæ˜ å°„åˆ°å›å¤å’ŒåŠ¨ä½œ
        if "happy" in emotion_lower or "joy" in emotion_lower or "smile" in emotion_lower:
            response_text = f"ä½ çœ‹èµ·æ¥å¾ˆå¼€å¿ƒå‘¢ï¼è®©æˆ‘ä»¬ä¸€èµ·å¼€å¿ƒå§ã€‚"
            action = "wave"
            action_type = "gesture"
            robot_emotion = "happy"
        
        elif "sad" in emotion_lower or "unhappy" in emotion_lower or "down" in emotion_lower:
            response_text = f"ä½ çœ‹èµ·æ¥æœ‰ç‚¹ä¸å¼€å¿ƒï¼Œéœ€è¦æˆ‘åšç‚¹ä»€ä¹ˆè®©ä½ å¼€å¿ƒå—ï¼Ÿ"
            action = "nod"
            action_type = "gesture"
            robot_emotion = "concerned"
        
        elif "angry" in emotion_lower or "mad" in emotion_lower:
            response_text = f"æˆ‘æ„Ÿè§‰åˆ°ä½ æœ‰ç‚¹ç”Ÿæ°”ï¼Œè®©æˆ‘ä»¬å†·é™ä¸€ä¸‹å§ã€‚"
            action = "bow"
            action_type = "gesture"
            robot_emotion = "apologetic"
        
        elif "surprise" in emotion_lower or "shocked" in emotion_lower:
            response_text = f"å“‡ï¼Œçœ‹èµ·æ¥å‘ç”Ÿäº†ä»€ä¹ˆè®©ä½ æƒŠè®¶çš„äº‹æƒ…ï¼"
            action = "thumbs_up"
            action_type = "gesture"
            robot_emotion = "excited"
        
        else:  # neutral or other
            response_text = f"ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚"
            action = "wave"
            action_type = "gesture"
            robot_emotion = "friendly"
        
        # æ ¹æ®ç”¨æˆ·è¯´çš„è¯è¿›ä¸€æ­¥è°ƒæ•´
        user_lower = user_text.lower()
        
        if "ä½ å¥½" in user_text or "hello" in user_lower or "hi" in user_lower:
            response_text = f"ä½ å¥½ï¼æˆ‘çœ‹åˆ°ä½ äº†ï¼Œ{gender_text(gender)}ã€‚" + response_text
            action = "wave"
        
        elif "å‰è¿›" in user_text or "forward" in user_lower or "èµ°" in user_text:
            response_text = "å¥½çš„ï¼Œæˆ‘ç°åœ¨å‘å‰ç§»åŠ¨ã€‚"
            action = "forward"
            action_type = "movement"
        
        elif "åœ" in user_text or "stop" in user_lower:
            response_text = "å¥½çš„ï¼Œæˆ‘åœä¸‹æ¥äº†ã€‚"
            action = "stop"
            action_type = "system"
        
        elif "æŒ¥æ‰‹" in user_text or "wave" in user_lower:
            response_text = "å¥½çš„ï¼Œæˆ‘å‘ä½ æŒ¥æ‰‹ï¼"
            action = "wave"
            action_type = "gesture"
        
        elif "ç‚¹å¤´" in user_text or "nod" in user_lower:
            response_text = "æ˜ç™½äº†ï¼"
            action = "nod"
            action_type = "gesture"
        
        return {
            "response_text": response_text,
            "action": action,
            "action_type": action_type,
            "robot_emotion": robot_emotion
        }
    
    def inference(self, image: np.ndarray, text: str) -> dict:
        """
        æ‰§è¡ŒVLMæ¨ç†ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
        
        å‚æ•°:
            image: OpenCVæ ¼å¼çš„å›¾åƒ (BGR, numpy array)
            text: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        
        è¿”å›:
            {
                "response_text": "æœºå™¨äººè¦è¯´çš„è¯",
                "action": "wave",
                "action_type": "gesture",
                "emotion": "happy",
                "confidence": 0.95,
                "analysis": {"age": 25, "gender": "male", "emotion": "happy"}
            }
        """
        logger.info(f"ğŸ§  å¼€å§‹VLMæ¨ç†ï¼ˆå¤šè½®å¯¹è¯ï¼‰...")
        logger.info(f"ğŸ“ ç”¨æˆ·è¾“å…¥: '{text}'")
        logger.info(f"ğŸ“š å½“å‰ä¸Šä¸‹æ–‡: {self.context.get_context_summary()}")
        logger.info(f"ğŸ“· å›¾åƒå°ºå¯¸: {image.shape}")
        
        try:
            # 1. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            self.context.add_user(text)
            
            # 2. è½¬æ¢å›¾åƒæ ¼å¼ (OpenCV BGR -> PIL RGB)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(image_rgb)
            
            # 3. ä¿å­˜ä¸´æ—¶å›¾åƒï¼ˆQwenéœ€è¦æ–‡ä»¶è·¯å¾„ï¼‰
            temp_image_path = "/tmp/thor_temp_image.jpg"
            pil_image.save(temp_image_path)
            img_path = f"file://{temp_image_path}"
            
            # 4. æ„å»ºprompt - åŒ…å«ä¸Šä¸‹æ–‡å’Œå½“å‰ä»»åŠ¡
            history_context = ""
            if self.context.history:
                history_context = "\n\n--- Recent Conversation History ---\n"
                for msg in self.context.history[-6:]:  # æœ€è¿‘3è½®ï¼ˆ6æ¡æ¶ˆæ¯ï¼‰
                    role = "User" if msg["role"] == "user" else "Robot"
                    history_context += f"{role}: {msg['content']}\n"
                history_context += "--- End of History ---\n\n"
            
            query = (
                f"{history_context}"
                f"Current user input: '{text}'\n\n"
                "Please analyze the person in this image. "
                "Estimate their approximate age (in years), gender, and emotional state "
                "based on visual cues. "
                "Output the result in JSON format as: "
                "{\"age\": <int>, \"gender\": <male/female>, \"emotion\": <string>}."
            )
            
            # 5. æ„å»ºæ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "image": img_path,
                            "resized_height": IMAGE_HEIGHT,
                            "resized_width": IMAGE_WIDTH
                        },
                        {"type": "text", "text": query}
                    ]
                }
            ]
            
            # 6. å¤„ç†è¾“å…¥
            text_prompt = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            image_inputs, video_inputs = process_vision_info(messages)
            inputs = self.processor(
                text=[text_prompt],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt"
            ).to(self.device)
            
            # 7. æ¨ç†
            logger.info("âš¡ æ‰§è¡Œæ¨¡å‹æ¨ç†...")
            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=256,
                    do_sample=False
                )
            
            # 8. è§£ç è¾“å‡º
            trimmed = [o[len(i):] for i, o in zip(inputs.input_ids, outputs)]
            output_text = self.processor.batch_decode(
                trimmed, skip_special_tokens=True
            )[0]
            
            logger.info(f"ğŸ“¤ æ¨¡å‹åŸå§‹è¾“å‡º: {output_text}")
            
            # 9. è§£æJSON
            analysis = self._parse_json_response(output_text)
            logger.info(f"ğŸ“Š åˆ†æç»“æœ: {analysis}")
            
            # 10. å†³å®šæœºå™¨äººçš„å›å¤å’ŒåŠ¨ä½œï¼ˆè€ƒè™‘ä¸Šä¸‹æ–‡ï¼‰
            decision = self._determine_action_and_response(analysis, text)
            
            # 11. æ·»åŠ æœºå™¨äººå›å¤åˆ°å†å²
            self.context.add_assistant(decision["response_text"])
            
            # 12. æ„å»ºæœ€ç»ˆå“åº”
            result = {
                "response_text": decision["response_text"],
                "action": decision["action"],
                "action_type": decision["action_type"],
                "emotion": decision["robot_emotion"],
                "confidence": 0.90,
                "analysis": analysis
            }
            
            logger.info(f"âœ… æ¨ç†å®Œæˆ: {result}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ VLMæ¨ç†å¤±è´¥: {e}", exc_info=True)
            # è¿”å›é»˜è®¤å“åº”
            return {
                "response_text": f"æˆ‘å¬åˆ°ä½ è¯´ï¼š{text}",
                "action": "nod",
                "action_type": "gesture",
                "emotion": "neutral",
                "confidence": 0.5,
                "error": str(e)
            }


def gender_text(gender: str) -> str:
    """æ€§åˆ«æ–‡æœ¬è½¬æ¢"""
    if gender == "male":
        return "å…ˆç”Ÿ"
    elif gender == "female":
        return "å¥³å£«"
    else:
        return "æœ‹å‹"


# ============================================================
# Flask åº”ç”¨å’Œå…¨å±€VLMæ¨¡å‹
# ============================================================
app = Flask(__name__)
vlm_model = None  # å…¨å±€æ¨¡å‹å®ä¾‹ï¼Œå¯åŠ¨æ—¶åˆå§‹åŒ–


# ============================================================
# HTTP API è·¯ç”±
# ============================================================
@app.route('/infer', methods=['POST'])
def infer():
    """
    VLMæ¨ç†APIç«¯ç‚¹
    
    è¯·æ±‚æ ¼å¼ (JSON):
        {
            "text": "ç”¨æˆ·è¯´çš„è¯",
            "image_base64": "base64ç¼–ç çš„å›¾åƒ",
            "request_id": "å¯é€‰çš„è¯·æ±‚ID",
            "timestamp": å¯é€‰çš„æ—¶é—´æˆ³
        }
    
    å“åº”æ ¼å¼ (JSON):
        {
            "status": "success",
            "text": "æœºå™¨äººå›å¤",
            "action": "wave",
            "action_type": "gesture",
            "emotion": "happy",
            "confidence": 0.95,
            "request_id": "è¯·æ±‚ID",
            "analysis": {...}
        }
    """
    try:
        # è§£æè¯·æ±‚æ•°æ®
        data = request.get_json()
        if not data:
            return jsonify({
                "status": "error",
                "error": "Invalid JSON",
                "text": "è¯·æ±‚æ ¼å¼é”™è¯¯"
            }), 400
        
        text = data.get("text", data.get("asr_text", ""))
        image_b64 = data.get("image_base64", "")
        request_id = data.get("request_id", "unknown")
        timestamp = data.get("timestamp", 0)
        
        logger.info("=" * 60)
        logger.info(f"ğŸ“¨ æ”¶åˆ°æ¨ç†è¯·æ±‚ (ID: {request_id[:8] if len(request_id) > 8 else request_id}...)")
        logger.info(f"ğŸ“ æ–‡æœ¬: '{text}'")
        logger.info(f"â±ï¸  æ—¶é—´æˆ³: {timestamp}")
        
        # è§£ç å›¾åƒ
        image = None
        if image_b64:
            try:
                img_data = base64.b64decode(image_b64)
                nparr = np.frombuffer(img_data, np.uint8)
                image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                logger.info(f"ğŸ“· å›¾åƒè§£ç æˆåŠŸ: {image.shape}")
            except Exception as e:
                logger.warning(f"âš ï¸ å›¾åƒè§£ç å¤±è´¥: {e}")
                image = np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH, 3), dtype=np.uint8)
        else:
            logger.warning("âš ï¸ æœªæ”¶åˆ°å›¾åƒï¼Œä½¿ç”¨ç©ºç™½å›¾åƒ")
            image = np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH, 3), dtype=np.uint8)
        
        # VLMæ¨ç†
        result = vlm_model.inference(image, text)
        
        # æ„å»ºå“åº”
        response = {
            "status": "success",
            "text": result["response_text"],
            "action": result["action"],
            "action_type": result["action_type"],
            "emotion": result["emotion"],
            "confidence": result["confidence"],
            "request_id": request_id,
            "analysis": result.get("analysis", {})
        }
        
        logger.info(f"âœ… æ¨ç†å®Œæˆ (ID: {request_id[:8] if len(request_id) > 8 else request_id}...)")
        logger.info("=" * 60)
        
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"âŒ æ¨ç†å¤±è´¥: {e}", exc_info=True)
        return jsonify({
            "status": "error",
            "error": str(e),
            "text": "æŠ±æ­‰ï¼Œå¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ã€‚",
            "action": "shake_head",
            "action_type": "gesture",
            "confidence": 0.0
        }), 500


@app.route('/health', methods=['GET'])
def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({
        "status": "healthy",
        "model_loaded": vlm_model is not None,
        "model_path": MODEL_PATH
    }), 200


# ============================================================
# ThoræœåŠ¡å™¨ä¸»ç±»
# ============================================================
class ThorVLMServer:
    """Thor VLM HTTPæ¨ç†æœåŠ¡å™¨"""
    
    def __init__(self):
        self.vlm_model = QwenVLMModel()
    
    def run(self, host: str = HOST, port: int = PORT, debug: bool = False):
        """
        å¯åŠ¨HTTPæœåŠ¡å™¨
        
        å‚æ•°:
            host: ç›‘å¬åœ°å€ (é»˜è®¤ 0.0.0.0 - æ‰€æœ‰æ¥å£)
            port: ç›‘å¬ç«¯å£ (é»˜è®¤ 5000)
            debug: æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼
        """
        global vlm_model
        vlm_model = self.vlm_model
        
        logger.info("=" * 60)
        logger.info("ğŸš€ Thor VLM HTTPæœåŠ¡å™¨ - å¯åŠ¨ä¸­")
        logger.info("=" * 60)
        logger.info(f"ğŸ“¡ ç›‘å¬åœ°å€: {host}:{port}")
        logger.info(f"ğŸ”— æ¨ç†ç«¯ç‚¹: POST http://{host}:{port}/infer")
        logger.info(f"ğŸ’š å¥åº·æ£€æŸ¥: GET http://{host}:{port}/health")
        logger.info("âŒ¨ï¸  æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
        logger.info("=" * 60)
        
        app.run(host=host, port=port, debug=debug, threaded=True)


# ============================================================
# ä¸»ç¨‹åºå…¥å£
# ============================================================
def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Thor VLM HTTPæ¨ç†æœåŠ¡å™¨ (åŸºäºQwen2.5-VL-3B-Instruct)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python thor_vlm_server.py
  python thor_vlm_server.py --host 0.0.0.0 --port 5000
  python thor_vlm_server.py --debug

æ³¨æ„:
  - ç¡®ä¿Qwen2.5-VLæ¨¡å‹è·¯å¾„æ­£ç¡®: {MODEL_PATH}
  - ç¡®ä¿G1æœºå™¨äººå¯ä»¥è®¿é—®æ­¤æœåŠ¡å™¨çš„IPå’Œç«¯å£
  - æ¨èä½¿ç”¨GPUåŠ é€Ÿ (CUDA)
        """.format(MODEL_PATH=MODEL_PATH)
    )
    
    parser.add_argument(
        "--host",
        default=HOST,
        help=f"ç›‘å¬åœ°å€ (é»˜è®¤: {HOST})",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=PORT,
        help=f"ç›‘å¬ç«¯å£ (é»˜è®¤: {PORT})",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¼€å¯è°ƒè¯•æ¨¡å¼",
    )
    
    args = parser.parse_args()
    
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("ğŸ› è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    logger.info("=" * 60)
    logger.info("ğŸ”§ ç³»ç»Ÿä¿¡æ¯")
    logger.info("=" * 60)
    logger.info(f"ğŸ–¥ï¸  CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        logger.info(f"ğŸ® GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
        logger.info(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    logger.info(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    logger.info(f"ğŸ“ å›¾åƒå°ºå¯¸: {IMAGE_WIDTH}x{IMAGE_HEIGHT}")
    logger.info("=" * 60)
    
    # åˆ›å»ºå¹¶è¿è¡ŒæœåŠ¡å™¨
    server = ThorVLMServer()
    server.run(host=args.host, port=args.port, debug=args.debug)


if __name__ == "__main__":
    main()
