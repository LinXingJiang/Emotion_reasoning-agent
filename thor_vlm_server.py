"""
Thor VLM æ¨ç†æœåŠ¡å™¨ - åŸºäº Qwen2.5-VL-3B-Instruct
åŠŸèƒ½: æ¥æ”¶G1å‘é€çš„å›¾åƒå’Œæ–‡æœ¬ï¼Œè¿›è¡ŒVLMæ¨ç†ï¼Œè¿”å›å“åº”
æ¨¡å‹: Qwen2.5-VL-3B-Instruct (äººç‰©åˆ†æã€æƒ…æ„Ÿè¯†åˆ«)
é€šä¿¡: HTTP/REST API (Flask)
"""

import json
import logging
import base64
import numpy as np
import cv2
import torch
import re
from PIL import Image
from flask import Flask, request, jsonify

from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

# ============================================================
# é…ç½®æ—¥å¿—
# ============================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# ============================================================
# é…ç½®å‚æ•°
# ============================================================
HOST = "0.0.0.0"
PORT = 5000

MODEL_PATH = "Qwen/Qwen2.5-VL-3B-Instruct"   # åœ¨çº¿æ¨¡å‹è·¯å¾„ï¼ˆè‡ªåŠ¨ç¼“å­˜ï¼‰
IMAGE_HEIGHT = 280
IMAGE_WIDTH = 420
PIXELS = IMAGE_HEIGHT * IMAGE_WIDTH


# ============================================================
# ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰
# ============================================================
class ContextManager:
    def __init__(self, max_turns: int = 10):
        self.max_turns = max_turns
        self.history = []
        logger.info(f"ğŸ“š ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆå§‹åŒ–: æœ€å¤§ä¿ç•™ {max_turns} è½®å¯¹è¯")

    def add_user(self, text: str):
        self.history.append({"role": "user", "content": text})
        self._trim()

    def add_assistant(self, text: str):
        self.history.append({"role": "assistant", "content": text})
        self._trim()

    def _trim(self):
        if len(self.history) > self.max_turns * 2:
            self.history = self.history[-self.max_turns * 2:]

    def get_history(self):
        return self.history.copy()

    def get_context_summary(self):
        if not self.history:
            return "æ— å†å²å¯¹è¯"
        turns = len(self.history) // 2
        return f"{turns}è½®å¯¹è¯ï¼Œæœ€è¿‘: {self.history[-1]['content'][:30]}..."


# ============================================================
# JSON æå–å‡½æ•°ï¼ˆé²æ£’ï¼‰
# ============================================================
def extract_json(text: str):
    # å°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(text)
    except:
        pass

    # æå–æ‰€æœ‰ { ... } å—ï¼Œé€‰æ‹©æœ€é•¿çš„å°è¯•
    candidates = re.findall(r"\{[\s\S]*?\}", text)
    for c in sorted(candidates, key=len, reverse=True):
        try:
            return json.loads(c)
        except:
            pass

    logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥ï¼Œè¾“å‡ºå†…å®¹: {text}")
    return {
        "age": 25,
        "gender": "unknown",
        "emotion": "neutral",
        "raw_output": text
    }


# ============================================================
# Qwen VLM æ¨¡å‹å°è£…
# ============================================================
class QwenVLMModel:
    def __init__(self, model_path: str = MODEL_PATH):
        logger.info("ğŸ”§ æ­£åœ¨åŠ è½½ Qwen2.5-VL-3B-Instruct æ¨¡å‹...")
        logger.info(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {model_path}")

        self.dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ–¥ï¸ è®¾å¤‡: {self.device}, æ•°æ®ç±»å‹: {self.dtype}")

        # åŠ è½½ Processor
        self.processor = AutoProcessor.from_pretrained(
            model_path,
            min_pixels=PIXELS,
            max_pixels=PIXELS,
            trust_remote_code=True,
            use_fast=False
        )

        # åŠ è½½æ¨¡å‹
        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=self.dtype,
            device_map="auto",
            # attn_implementation="flash_attention_2",  # åŠ é€Ÿæ³¨æ„åŠ›
            trust_remote_code=True
        ).eval()

        self.context = ContextManager(max_turns=10)
        logger.info("âœ… Qwen2.5-VL æ¨¡å‹åŠ è½½å®Œæˆï¼")

    # ======================= è§†è§‰æ¨ç† ===========================
    def inference(self, image: np.ndarray, text: str):
        logger.info("ğŸ§  å¼€å§‹VLMæ¨ç†...")
        logger.info(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {text}")
        logger.info(f"ğŸ“š ä¸Šä¸‹æ–‡: {self.context.get_context_summary()}")

        self.context.add_user(text)

        # ----------- å›¾åƒè½¬æ¢ï¼ˆOpenCV â†’ PILï¼‰-----------
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(image_rgb)

        # ----------- æ„å»º promptï¼ˆhistory ä¸è§†è§‰ä»»åŠ¡éš”ç¦»ï¼‰-----------
        history_context = ""
        if self.context.history:
            history_context += "### Conversation History\n"
            for msg in self.context.history[-6:]:
                role = "User" if msg["role"] == "user" else "Robot"
                history_context += f"{role}: {msg['content']}\n"
            history_context += "### End of History\n\n"

        query = (
            f"{history_context}"
            "### Visual Task\n"
            "Analyze the person in the image and output a JSON object:\n"
            "{\"age\": <int>, \"gender\": <male/female>, \"emotion\": <string>}."
        )

        # ----------- æ„å»ºæ¶ˆæ¯ï¼ˆä¸å†ä½¿ç”¨ file:// ï¼‰-----------
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": pil_image,
                     "resized_height": IMAGE_HEIGHT, "resized_width": IMAGE_WIDTH},
                    {"type": "text", "text": query},
                ]
            }
        ]

        text_prompt = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)

        inputs = self.processor(
            text=[text_prompt],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        ).to(self.device)

        # ----------- æ¨¡å‹æ¨ç† -----------
        with torch.inference_mode():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=False
            )

        trimmed = [o[len(i):] for i, o in zip(inputs.input_ids, outputs)]
        output_text = self.processor.batch_decode(trimmed, skip_special_tokens=True)[0]
        logger.info(f"ğŸ“¤ æ¨¡å‹è¾“å‡º: {output_text}")

        analysis = extract_json(output_text)

        # ----------- å†³å®šåŠ¨ä½œä¸å›åº” -----------
        decision = determine_action_and_response(analysis, text)
        self.context.add_assistant(decision["response_text"])

        return {
            "response_text": decision["response_text"],
            "action": decision["action"],
            "action_type": decision["action_type"],
            "emotion": decision["robot_emotion"],
            "confidence": 0.90,
            "analysis": analysis
        }


# ============================================================
# æœºå™¨äººåŠ¨ä½œé€»è¾‘
# ============================================================
def gender_text(gender: str) -> str:
    if gender == "male": return "å…ˆç”Ÿ"
    if gender == "female": return "å¥³å£«"
    return "æœ‹å‹"


def determine_action_and_response(analysis: dict, user_text: str) -> dict:
    emotion = analysis.get("emotion", "").lower()
    response = ""
    action = "idle"
    action_type = "gesture"
    robot_emotion = "neutral"

    # ===== Emotion-based polite responses =====
    if "happy" in emotion:
        response = "You look happy today."
        action = "wave"
        robot_emotion = "happy"

    elif "sad" in emotion:
        response = "You seem a bit sad. Iâ€™m here if you need support."
        action = "nod"
        robot_emotion = "concerned"

    elif "angry" in emotion or "mad" in emotion:
        response = "I notice some signs of anger. Please take your time."
        action = "bow"
        robot_emotion = "apologetic"

    elif "surprise" in emotion:
        response = "You appear surprised."
        action = "thumbs_up"
        robot_emotion = "neutral"

    else:
        response = "Hello. Itâ€™s good to see you."
        action = "wave"
        robot_emotion = "friendly"

    # ===== Parse user commands (English only) =====
    u = user_text.lower()

    # Movement controls
    if "forward" in u or "move forward" in u:
        return {
            "response_text": "Moving forward.",
            "action": "forward",
            "action_type": "movement",
            "robot_emotion": robot_emotion
        }

    if "back" in u or "backward" in u:
        return {
            "response_text": "Moving backward.",
            "action": "backward",
            "action_type": "movement",
            "robot_emotion": robot_emotion
        }

    if "left" in u or "turn left" in u:
        return {
            "response_text": "Turning left.",
            "action": "turn_left",
            "action_type": "movement",
            "robot_emotion": robot_emotion
        }

    if "right" in u or "turn right" in u:
        return {
            "response_text": "Turning right.",
            "action": "turn_right",
            "action_type": "movement",
            "robot_emotion": robot_emotion
        }

    if "stop" in u or "halt" in u:
        return {
            "response_text": "Stopping now.",
            "action": "stop",
            "action_type": "system",
            "robot_emotion": robot_emotion
        }

    # Gesture controls
    if "wave" in u:
        return {
            "response_text": "Waving now.",
            "action": "wave",
            "action_type": "gesture",
            "robot_emotion": robot_emotion
        }

    if "nod" in u:
        return {
            "response_text": "Nodding.",
            "action": "nod",
            "action_type": "gesture",
            "robot_emotion": robot_emotion
        }

    return {
        "response_text": response,
        "action": action,
        "action_type": action_type,
        "robot_emotion": robot_emotion
    }

# ============================================================
# Flask æœåŠ¡
# ============================================================
app = Flask(__name__)
vlm_model = None


@app.route("/infer", methods=["POST"])
def infer():
    data = request.get_json()
    if not data:
        return jsonify({"status": "error", "message": "Invalid JSON"}), 400

    text = data.get("text", data.get("asr_text", ""))
    image_b64 = data.get("image_base64", "")
    request_id = data.get("request_id", "unknown")

    logger.info("=" * 60)
    logger.info(f"ğŸ“¨ æ”¶åˆ°æ¨ç†è¯·æ±‚ ID={request_id}")

    if not image_b64:
        return jsonify({"status": "error", "message": "æœªæ”¶åˆ°å›¾åƒ"}), 400

    # è§£ç å›¾åƒ
    try:
        img_data = base64.b64decode(image_b64)
        nparr = np.frombuffer(img_data, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if image is None:
            raise ValueError("cv2 decode returned None")
    except Exception as e:
        logger.error(f"âŒ å›¾åƒè§£ç å¤±è´¥: {e}")
        return jsonify({"status": "error", "message": "å›¾åƒè§£ç å¤±è´¥"}), 400

    result = vlm_model.inference(image, text)

    return jsonify({
        "status": "success",
        "text": result.get("response_text", ""),
        "action": result.get("action", ""),
        "action_type": result.get("action_type", ""),
        "emotion": result.get("emotion", "neutral"),
        "confidence": result.get("confidence", 0.0),
        "analysis": result.get("analysis", {}),
        "request_id": request_id
    })


@app.route("/health", methods=["GET"])
def health():
    return jsonify({
        "status": "healthy",
        "model_loaded": vlm_model is not None,
        "model_path": MODEL_PATH
    })


# ============================================================
# å¯åŠ¨æœåŠ¡å™¨
# ============================================================
class ThorVLMServer:
    def __init__(self):
        global vlm_model
        vlm_model = QwenVLMModel()

    def run(self, host=HOST, port=PORT):
        logger.info("ğŸš€ Thor VLM HTTP æœåŠ¡å™¨å¯åŠ¨ï¼")
        app.run(host=host, port=port, threaded=True)


def main():
    server = ThorVLMServer()
    server.run()


if __name__ == "__main__":
    main()